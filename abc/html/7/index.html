
    <!doctype html><html lang="en">
    <head>
        <meta charset="utf-8">
        <title> Kubernetes ABC - LAB 7</title>
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../main.css">
        <script src="../highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
    </head>
    <body>
    <header>
        <div>
            <span> Kubernetes ABC - LAB 7</span>
            <img src="../logo.svg"/>
        </div>
    </header>
    <div class="body"><h3 id="1emptydir">1) Empty Dir</h3>
<pre><code class="shell language-shell">$ cd ~/7
</code></pre>
<p><details class="cmd-file"><summary>empty_dir.yaml <span class="show-text">&plus;</span></summary><pre><code class="yaml language-yaml">apiVersion: v1
kind: Service
metadata:
  name: emptydir
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: emptydir
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: emptydir
  name: emptydir
spec:
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: emptydir
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: emptydir
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: test
      - image: ubuntu
        volumeMounts:
        - mountPath: /busy
          name: test
        name: ubuntu
        env:
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        command:
        - sh
        - -c
        - chmod 777 /busy &amp;&amp; echo "$MY_POD_NAMESPACE - $MY_NODE_NAME - $MY_POD_NAME" &gt;&gt; /busy/index.html  &amp;&amp; while true; do sleep 1 &amp;&amp; date; done
      restartPolicy: Always
      terminationGracePeriodSeconds: 3
      volumes:
       - name: test
         emptyDir: {} 

</code></pre>
</details></p>
<p>It will create a service for the deployment and a deployment with two pods. Each pod has two containers. <br/>
The pod has defined a volume named <strong>test</strong> and the type is <strong>emptyDir</strong>.</p>
<p>In nginx container it is mounted under /usr/share/nginx/html and ubuntu container /busy.  <br/>
Pay attention to the ubuntu pods start up command. It will write an index.html file to /busy that contains the namespace, worker node hostname and pod name. </p>
<pre><code class="shell language-shell">$ kubectl create -f empty_dir.yaml
$ kubectl get pods -o wide
</code></pre>
<p>Please note also the worker node the pod is launched on.</p>
<p>From the last lab you should still have <strong>client</strong> pod. If you don't have it for some reason, use kubectl create -f ~/6/client.yaml </p>
<p>Open a terminal in the client pod and access the emptydir service.</p>
<pre><code class="shell language-shell">$ kubectl exec -it client bash
# curl emptydir
</code></pre>
<p>Do this several times.</p>
<pre><code class="shell language-shell"># exit
</code></pre>
<p>Now log into the first emptydir pod:</p>
<pre><code class="shell language-shell">$ kubectl exec -it &lt;first pod&gt; -c ubuntu bash
# echo "I am pod one" &gt;&gt; /busy/index.html
# exit
$ kubectl exec -it &lt;second pod&gt; -c ubuntu bash
# echo "I am pod two" &gt;&gt; /busy/index.html
# exit
</code></pre>
<p>From the client pod, do the curl commands again:</p>
<pre><code class="shell language-shell">$ kubectl exec -it client bash
# curl emptydir
</code></pre>
<p>You will see different messages depending on what pod the request was sent to.   <br/>
The empty dir is shared between the nginx and ubuntu containers - this is how nginx can serve the index.html file.  <br/>
It is not shared between pods - even if they happen to run on the same worker node.</p>
<p>Delete the first emptydir pod and check pods.</p>
<p><details class="cmd-one-line"><summary>Show command</summary><pre><code class="shell language-shell">$ kubectl delete pod &lt;first emptydir pod&gt;
</code></pre>
</details></p>
<p><details class="cmd-one-line"><summary>Show command</summary><pre><code class="shell language-shell">$ kubectl get pods -o wide
</code></pre>
</details></p>
<p>Repeat the curl on client. <br/>
This shows that emptyDir will not survive deletion since your custom message will not be available on the newly launched pod.</p>
<p>Open a shell in the other pod (the older one) and kill nginx process.</p>
<pre><code class="shell language-shell">$ kubectl exec -it &lt;second emptydir pod&gt; -c nginx sh
# kill 1
$ kubectl get pods -o wide
</code></pre>
<p>You should see the Restarts count increased on the pod where you killed nginx. <br/>
When using curl again you will see that the restarted pod still has your custom message. <br/>
This shows that emptyDir will survive container restarts.</p>
<p>When using emptyDir you should not assume that it is always empty when the container starts.</p>
<h3 id="2hostpath">2) Host path</h3>
<p><details class="cmd-file"><summary>host_path.yaml <span class="show-text">&plus;</span></summary><pre><code class="yaml language-yaml">apiVersion: v1
kind: Service
metadata:
  name: hostpath
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hostpath
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hostpath
  name: hostpath
spec:
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: hostpath
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: hostpath
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: test
      - image: ubuntu
        volumeMounts:
        - mountPath: /busy
          name: test
        name: ubuntu
        env:
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        command:
        - sh
        - -c
        - chmod 777 /busy &amp;&amp; echo "$MY_POD_NAMESPACE - $MY_NODE_NAME - $MY_POD_NAME" &gt;&gt; /busy/index.html  &amp;&amp; while true; do sleep 1 &amp;&amp; date; done
      restartPolicy: Always
      terminationGracePeriodSeconds: 3
      volumes:
       - name: test
         hostPath:
           path: /mnt/hostpath
           type: DirectoryOrCreate

</code></pre>
</details></p>
<p>It is the same as the last deployment, but the volume type is <strong>hostpath</strong> and it is pointed to /mnt/hostpath on the node.
Create deployment with file host_path.yaml and check pods.</p>
<p><details class="cmd-one-line"><summary>Show command</summary><pre><code class="shell language-shell">$ kubectl create -f host_path.yaml
</code></pre>
</details></p>
<p><details class="cmd-one-line"><summary>Show command</summary><pre><code class="shell language-shell">$ kubectl get pods -o wide
</code></pre>
</details></p>
<p>The volume will be mounted from the node that the pod is started on. If two pods happen to be on the same node, the pods would have a shared folder. <br/>
After the pods are up, do a curl many times. You should see other users' namespaces and pod names in the response. <br/>
This is because the nodes folder is shared between all the pods that run on that node.</p>
<pre><code class="shell language-shell">$ kubectl exec -it client bash
# curl hostpath
</code></pre>
<p>When you kill a pod then it might spawn on another node and adding its data to another node. <br/>
Kill any of the hostpath pods and check pods.</p>
<p><details class="cmd-one-line"><summary>Show command</summary><pre><code class="shell language-shell">$ kubectl delete pod &lt;hostpath&gt;
</code></pre>
</details></p>
<p><details class="cmd-one-line"><summary>Show command</summary><pre><code class="shell language-shell">$ kubectl get pods -o wide
</code></pre>
</details></p>
<p>Repeat the curl.</p>
<p>The paths storage can be persistent. This is the easiest way to have persistent storage - of course you would need to make sure the pod sticks to one specific node. <br/>
The downside is that it would be a single point of failure - when the worker node dies you won't be able to access your data.</p>
<h3 id="3cleanupthelab">3) Clean up the lab</h3>
<pre><code class="shell language-shell">$ kubectl delete deployment --all
$ kubectl delete svc --all
</code></pre></div><script src="../main.js"></script></body></html>